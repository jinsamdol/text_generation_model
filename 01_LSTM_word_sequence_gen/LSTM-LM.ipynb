{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (character-wise) Language modeling  \n",
    "\n",
    "##### Author: Wangjin Lee, Seoul National University. jinsamdol@snu.ac.kr  \n",
    "http://nlpway.wordpress.com  \n",
    "\n",
    "##### References  \n",
    "* Tomas Mikolov et al. \"Recurrent neural network based language model\". INTERSPEECH 2010.  \n",
    "* Martin Sundermeyer, Ralf Schluter, and Hermann Ney. \"LSTM Neural networks for language modeling\"  \n",
    "* Andrej Karpathy. \"The unreasonable effectivenss of recurrent neural networks\". 2015.  \n",
    "* Andrej Karpathy. char-rnn. https://github.com/karpathy/char-rnn#tips-and-tricks  \n",
    "  \n",
    "This code is a simple LSTM language model predicting following three characters given character sequence.  \n",
    "Vocabulary is a set of alphabets, and they are used in the form of one-hot encoded sequence in the graph.  \n",
    "  \n",
    "Some notables points are as following:  \n",
    "* the source and target (x , y) have to have same length.  \n",
    "* the logit is calculated based on cross-entropy for measuring the difference between the prediction and the real target value.  \n",
    "* gradient clipping was used in order to prevent gradient exploding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ['abcde', 'bcdef', 'cdefg', 'defgh', 'efghi', 'fghij', 'ghijk', 'hijkl', 'ijklm', 'jklmn', 'klmno', 'lmnop', 'mnopq', 'nopqr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {c: i for i, c in enumerate(voca)}\n",
    "int_to_vocab = {}\n",
    "for v, i in vocab_to_int.items():\n",
    "    #print(i, v)\n",
    "    int_to_vocab[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7], [4, 5, 6, 7, 8], [5, 6, 7, 8, 9], [6, 7, 8, 9, 10], [7, 8, 9, 10, 11], [8, 9, 10, 11, 12], [9, 10, 11, 12, 13], [10, 11, 12, 13, 14], [11, 12, 13, 14, 15], [12, 13, 14, 15, 16], [13, 14, 15, 16, 17]]\n",
      "[[ 0  1  2  3  4]\n",
      " [ 1  2  3  4  5]\n",
      " [ 2  3  4  5  6]\n",
      " [ 3  4  5  6  7]\n",
      " [ 4  5  6  7  8]\n",
      " [ 5  6  7  8  9]\n",
      " [ 6  7  8  9 10]\n",
      " [ 7  8  9 10 11]\n",
      " [ 8  9 10 11 12]\n",
      " [ 9 10 11 12 13]\n",
      " [10 11 12 13 14]\n",
      " [11 12 13 14 15]\n",
      " [12 13 14 15 16]\n",
      " [13 14 15 16 17]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "encoded_source = []\n",
    "for s in source:    \n",
    "    tmp = [vocab_to_int[i] for i in s]\n",
    "    encoded_source.append(tmp)\n",
    "    \n",
    "print(encoded_source)\n",
    "encoded_source = np.array(encoded_source, dtype=np.int32)\n",
    "print(encoded_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 20\n",
    "kp = 0.6\n",
    "batch_size = 1\n",
    "grad_clip = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None], name='input')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, None], name='output')\n",
    "    \n",
    "    x_one_hot = tf.one_hot(x, len(voca))\n",
    "    \n",
    "    def build_cell(rnn_size, kp):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=kp)\n",
    "    \n",
    "    #hidden layer\n",
    "    with tf.variable_scope('lstm_cell'):\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, kp) for _ in range(2)])\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        #output of the hidden layer\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "\n",
    "    #The output is a bunch of rows, one row for each step for each sequence. \n",
    "    #Concatenate lstm output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    seq_output = tf.reshape(seq_output, [-1, rnn_size])\n",
    "    \n",
    "    #prediction\n",
    "    with tf.variable_scope('prediction'):        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((rnn_size, len(voca)), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(len(voca)))\n",
    "        \n",
    "        logits = tf.matmul(seq_output, softmax_w)+softmax_b\n",
    "        prediction = tf.nn.softmax(logits, name='my_softmax')\n",
    "        \n",
    "    #loss\n",
    "    with tf.variable_scope('loss'):\n",
    "        y_one_hot = tf.one_hot(y, len(voca))\n",
    "        y_reshape = tf.reshape(y_one_hot, tf.shape(logits)) #one row per sequence per step.\n",
    "        #tf.shape(logits) error x , logits.get_shape() error o\n",
    "        \n",
    "        loss = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshape) )\n",
    "        #loss = tf.reduce_mean(loss)\n",
    "        \n",
    "    #op\n",
    "    with tf.variable_scope('optimizer'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 2.8926013\n",
      "Epoch: 100, loss: 0.99215025\n",
      "Epoch: 200, loss: 0.5175403\n",
      "Epoch: 300, loss: 0.25197208\n",
      "Epoch: 400, loss: 0.22179493\n",
      "Epoch: 500, loss: 0.1121848\n",
      "Epoch: 600, loss: 0.05812817\n",
      "Epoch: 700, loss: 0.050513845\n",
      "Epoch: 800, loss: 0.0913099\n",
      "Epoch: 900, loss: 0.07065218\n",
      "Epoch: 1000, loss: 0.03791935\n",
      "Epoch: 1100, loss: 0.05409404\n",
      "Epoch: 1200, loss: 0.06788659\n",
      "Epoch: 1300, loss: 0.110768475\n",
      "Epoch: 1400, loss: 0.07385433\n",
      "Epoch: 1500, loss: 0.08666229\n",
      "Epoch: 1600, loss: 0.03506421\n",
      "Epoch: 1700, loss: 0.10255342\n",
      "Epoch: 1800, loss: 0.04191437\n",
      "Epoch: 1900, loss: 0.045337975\n",
      "Epoch: 2000, loss: 0.043649424\n"
     ]
    }
   ],
   "source": [
    "print_step = 100\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(2001):\n",
    "        tl = []\n",
    "        for single_source in encoded_source:\n",
    "            source = [single_source[:3]]\n",
    "            target = [single_source[2:]]\n",
    "            #print(source)\n",
    "            #print(target)\n",
    "            train_loss = sess.run([loss, optimizer], feed_dict={x:source, y:target})\n",
    "            \n",
    "            tl.append(train_loss[0])\n",
    "        if e % print_step==0:\n",
    "            print('Epoch: {}, loss: {}'.format(e, str(np.mean(tl))))\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, './lstm-LM')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lstm-LM\n",
      "input: abc\n",
      "output: cde\n",
      "input: bcd\n",
      "output: def\n",
      "input: cde\n",
      "output: efg\n",
      "input: def\n",
      "output: fgh\n",
      "input: efg\n",
      "output: ghi\n",
      "input: fgh\n",
      "output: hij\n",
      "input: ghi\n",
      "output: ijk\n",
      "input: hij\n",
      "output: jkl\n",
      "input: ijk\n",
      "output: klm\n",
      "input: jkl\n",
      "output: lmn\n",
      "input: klm\n",
      "output: mno\n",
      "input: lmn\n",
      "output: nop\n",
      "input: mno\n",
      "output: opq\n",
      "input: nop\n",
      "output: pqr\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "restored_graph = tf.Graph()\n",
    "with restored_graph.as_default():\n",
    "    with tf.Session(graph = restored_graph) as sess:\n",
    "\n",
    "        loader = tf.train.import_meta_graph('./lstm-LM.meta')\n",
    "        loader.restore(sess, './lstm-LM')\n",
    "        \n",
    "        '''\n",
    "        for v in restored_graph.get_operations():\n",
    "            print(v)\n",
    "        '''\n",
    "        x = restored_graph.get_tensor_by_name('input:0')\n",
    "        my_prediction = restored_graph.get_tensor_by_name('prediction/my_softmax:0')\n",
    "        \n",
    "        for single_source in encoded_source:\n",
    "            source = [single_source[:3]]\n",
    "            out = sess.run([my_prediction], feed_dict={x:source})\n",
    "            _in = \"input: \"\n",
    "            for s in source[0]:\n",
    "                _in += int_to_vocab[s]\n",
    "                \n",
    "            print(_in)\n",
    "            \n",
    "            ostr = \"output: \"\n",
    "            for _o in out[0]:\n",
    "                #print(_o)\n",
    "                i = np.argmax(_o)\n",
    "                #print(i)\n",
    "                #print(int_to_vocab[i])\n",
    "                ostr+=str(int_to_vocab[i])\n",
    "            print(ostr)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
